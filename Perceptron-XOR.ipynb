{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d59fc3-bab3-4faa-92e4-6c42dff62fd6",
   "metadata": {},
   "source": [
    "## Computational Intelegence\n",
    "## Learning Xor with Perceptron\n",
    "#### Ali Soltan Mohammd\n",
    "#### ali.s.mohammdi@znu.ac.ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264740a9-a205-42d3-975b-c0bf89bf51b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLearning XOR with a single-layer perceptron is not possible due to its linear nature. The XOR function is not linearly separable,\\nwhich means a single perceptron cannot learn to classify XOR inputs correctly.In the new approach using a single-layer perceptron\\nto learn XOR, we transform the XOR problem into a linearly separable one,making it possible for a single-layer perceptron to learn XOR.\\nTo overcome this limitation, more complex models like multi-layer perceptrons or neural networks with hidden layers are needed \\nto successfully learn the XOR function.These models can capture the non-linear relationships present in the XOR problem, \\nmaking it possible to train a neural network to accurately predict XOR outputs.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Learning XOR with a single-layer perceptron is not possible due to its linear nature. The XOR function is not linearly separable,\n",
    "which means a single perceptron cannot learn to classify XOR inputs correctly.In the new approach using a single-layer perceptron\n",
    "to learn XOR, we transform the XOR problem into a linearly separable one,making it possible for a single-layer perceptron to learn XOR.\n",
    "To overcome this limitation, more complex models like multi-layer perceptrons or neural networks with hidden layers are needed \n",
    "to successfully learn the XOR function.These models can capture the non-linear relationships present in the XOR problem, \n",
    "making it possible to train a neural network to accurately predict XOR outputs.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d31112-c42c-47c6-80dd-c84aa482f6e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****   step 1 for calculating P1   ****\n",
      "w1:  2, w3:  2, b1:  -2\n",
      "w1:  2, w3:  2, b1:  -2\n",
      "w1:  2, w3:  2, b1:  -2\n",
      "w1:  2, w3:  2, b1:  -2\n",
      "y1 in epoch 0: [ 1 -1 -1  1] \n",
      "\n",
      "w1:  2, w3:  2, b1:  -2\n",
      "w1:  2, w3:  2, b1:  -2\n",
      "w1:  2, w3:  2, b1:  -2\n",
      "w1:  2, w3:  2, b1:  -2\n",
      "y1 in epoch 1: [-1 -1 -1  1] \n",
      "\n",
      "y1 final:  [-1 -1 -1  1]\n",
      "\n",
      "****   step 2 for calculating P2   ****\n",
      "w2:  2, w4:  2, b2:  -2\n",
      "w2:  0, w4:  4, b2:  0\n",
      "w2:  2, w4:  2, b2:  2\n",
      "w2:  2, w4:  2, b2:  2\n",
      "y2 in epoch 0: [ 1 -1 -1  1] \n",
      "\n",
      "w2:  2, w4:  2, b2:  2\n",
      "w2:  2, w4:  2, b2:  2\n",
      "w2:  2, w4:  2, b2:  2\n",
      "w2:  2, w4:  2, b2:  2\n",
      "y2 in epoch 1: [-1  1  1  1] \n",
      "\n",
      "y2 final:  [-1  1  1  1]\n",
      "\n",
      "****   step 3 for calculating P3   ****\n",
      "w5:  2, w6:  2, b3:  -2\n",
      "w5:  0, w6:  4, b3:  0\n",
      "w5:  0, w6:  4, b3:  0\n",
      "w5:  -2, w6:  2, b3:  -2\n",
      "z in epoch 0: [ 1 -1  1  1] \n",
      "\n",
      "w5:  -2, w6:  2, b3:  -2\n",
      "w5:  -2, w6:  2, b3:  -2\n",
      "w5:  -2, w6:  2, b3:  -2\n",
      "w5:  -2, w6:  2, b3:  -2\n",
      "z in epoch 1: [-1  1  1 -1] \n",
      "\n",
      "z final:  [-1  1  1 -1]\n",
      "\n",
      "w1: 2  w2: 2  w3: 2  w4: 2  w5: -2  w6: 2  b1: -2  b2: 2  b3: -2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y1 = np.array([0, 0, 0, 0])\n",
    "y2 = np.array([0, 0, 0, 0])\n",
    "z = np.array([0, 0, 0, 0])\n",
    "# Activation function (sign function)\n",
    "def sign(x):\n",
    "    return np.where(x >= 0, 1, -1)\n",
    "\n",
    "# Perceptron class\n",
    "class Perceptron:\n",
    "    def __init__(self):\n",
    "        # Initialize weights and biases\n",
    "        self.w1 = 0\n",
    "        self.w2 = 0\n",
    "        self.w3 = 0\n",
    "        self.w4 = 0\n",
    "        self.w5 = 0\n",
    "        self.w6 = 0\n",
    "        self.b1 = 0\n",
    "        self.b2 = 0\n",
    "        self.b3 = 0\n",
    "\n",
    "    def train_step1(self, inputs, targets1, tetha=0.1, alpha=1, epochs=10):\n",
    "        print(\"****   step 1 for calculating P1   ****\")\n",
    "        for epoch in range(epochs):\n",
    "            error = 0\n",
    "            for i in range(len(inputs)):\n",
    "                x1, x2 = inputs[i]\n",
    "                target = targets1[i]\n",
    "\n",
    "                # Forward pass\n",
    "                y1[i] = sign(self.w1 * x1 + self.w3 * x2 + self.b1)\n",
    "\n",
    "                # Update weights and biases for step 1\n",
    "                if target != y1[i]:\n",
    "                    self.w1 += alpha * (target - y1[i]) * x1\n",
    "                    self.w3 += alpha * (target - y1[i]) * x2\n",
    "                    self.b1 += alpha * (target - y1[i])\n",
    "                error += (target - y1[i]) ** 2\n",
    "                #print(\"Step 1 weights:\", p1.w1, p1.w3, p1.b1)\n",
    "                print(\"w1:  {}, w3:  {}, b1:  {}\".format(p1.w1, p1.w3, p1.b1))\n",
    "            mean_squared_error = error / len(inputs)\n",
    "            print(\"y1 in epoch {}: {} \".format(epoch, y1))\n",
    "            print()\n",
    "            if mean_squared_error < tetha:\n",
    "                break            \n",
    "\n",
    "        print(\"y1 final: \", y1)\n",
    "        print()\n",
    "    def train_step2(self, inputs, targets2, tetha=0.1, alpha=1, epochs=10):\n",
    "        print(\"****   step 2 for calculating P2   ****\")\n",
    "        for epoch in range(epochs):\n",
    "            error = 0\n",
    "            for i in range(len(inputs)):\n",
    "                x1, x2 = inputs[i]\n",
    "                target = targets2[i]\n",
    "\n",
    "                # Forward pass\n",
    "                #y2 = self.forward(x1, x2)\n",
    "                y2[i] = sign(self.w2 * x1 + self.w4 * x2 + self.b2)\n",
    "                \n",
    "                # Update weights and biases for step 2\n",
    "                if target != y2[i]:\n",
    "                    self.w2 += alpha * (target - y2[i]) * x1\n",
    "                    self.w4 += alpha * (target - y2[i]) * x2\n",
    "                    self.b2 += alpha * (target - y2[i])\n",
    "                error += (target - y2[i]) ** 2\n",
    "                #print(\"Step 2 weights:\", p2.w2, p2.w4, p2.b2)\n",
    "                print(\"w2:  {}, w4:  {}, b2:  {}\".format(p2.w2, p2.w4, p2.b2))\n",
    "            mean_squared_error = error / len(inputs)\n",
    "            print(\"y2 in epoch {}: {} \".format(epoch, y2))\n",
    "            print()\n",
    "            if mean_squared_error < tetha:\n",
    "                break\n",
    "        print(\"y2 final: \", y2)\n",
    "        print()\n",
    "    \n",
    "    def train_step3(self, inputs, targets3, tetha=0.1, alpha=1, epochs=10):\n",
    "        print(\"****   step 3 for calculating P3   ****\")\n",
    "        for epoch in range(epochs):\n",
    "            error = 0\n",
    "            for i in range(len(inputs)):\n",
    "                x1, x2 = inputs[i]\n",
    "                target = targets3[i]\n",
    "\n",
    "                # Forward pass\n",
    "                z[i] = sign(self.w5 * y1[i] + self.w6 * y2[i] + self.b3)\n",
    "                \n",
    "                # Update weights and biases for step 3\n",
    "                if target != z[i]:\n",
    "                    self.w5 += alpha * (target - z[i]) * y1[i]\n",
    "                    self.w6 += alpha * (target - z[i]) * y2[i]\n",
    "                    self.b3 += alpha * (target - z[i])\n",
    "                error += (target - z[i]) ** 2\n",
    "                print(\"w5:  {}, w6:  {}, b3:  {}\".format(p3.w5, p3.w6, p3.b3))\n",
    "            mean_squared_error = error / len(inputs)\n",
    "            print(\"z in epoch {}: {} \".format(epoch, z))\n",
    "            print()\n",
    "            if mean_squared_error < tetha:\n",
    "                break\n",
    "\n",
    "        print(\"z final: \", z)\n",
    "        print()\n",
    "\n",
    "# Define input and target data\n",
    "inputs = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
    "targets1 = np.array([-1, -1, -1, 1])\n",
    "targets2 = np.array([-1, 1, 1, 1])\n",
    "targets3 = np.array([-1, 1, 1, -1])\n",
    "\n",
    "# Create perceptron objects\n",
    "p1 = Perceptron()\n",
    "p2 = Perceptron()\n",
    "p3 = Perceptron()\n",
    "\n",
    "# Train the perceptrons\n",
    "p1.train_step1(inputs, targets1)\n",
    "p2.train_step2(inputs, targets2)\n",
    "p3.train_step3(inputs, targets3)\n",
    "\n",
    "# Print the weights and biases of each step\n",
    "print(\"w1: {}  w2: {}  w3: {}  w4: {}  w5: {}  w6: {}  b1: {}  b2: {}  b3: {}\".format(p1.w1, p2.w2, p1.w3, p2.w4, p3.w5, p3.w6, p1.b1, p2.b2, p3.b3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6bb39-1623-4eb6-8bb6-42c33bc816c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
